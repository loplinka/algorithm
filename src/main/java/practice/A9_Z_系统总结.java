/**
 * Hailiang.com Inc.
 * Copyright (c) 2004-2023 All Rights Reserved.
 */
package practice;

/**
 *
 * @author Baojiang Yang
 * @version : A9_Z_系统总结.java, v 0.1 2023年12月06日 08:55  Baojiang Yang Exp $
 */
public class A9_Z_系统总结 {
    /**
     *
     * 一.规则引擎系统
     *
     * 背景?
     * 老的模型系统是一个第三方依赖的方式集成在其他系统中,这个系统本身除了规则计算还承担了其他诸如可带额度咨询、记账、机构资金池管理等职能,经过大促和业务的长期发展后,
     * 逐渐暴露出以下弊端:
     * 1.系统职能不清晰,隔离性差,系统启动慢,模型本身不支持灰度发布,包括本地测试用例慢等
     * 2.双十一大促下系统资源得不到充分利用,(因为一味地扩容,资源没有得到最大化的利用,而规则引擎本身的逻辑并不复杂而且相对独立,启动耗时运行时运行逻辑又简单)
     *
     * 目标?
     * 为解决上述目标问题,就进行规则引擎系统的重建,达到以下的目的:
     * 1.解决原有痛点问题: 职能清晰,隔离性强,系统启动慢无法改变但是不拖慢原决策系统,支持灰度切流(原应用发布支持机房,机器,逻辑单元等灰度发布,
     *      此处的灰度切流是指模型上线后,老版本模型切换到新版本模型的灰度,因为是动态加载,不存在规则应用本身的发布嘛)
     * 2.集成新版本的风控大脑平台,进行规则的开发和管理,这个是探索性的
     * 基于以上3点,新模型系统就开始建设
     *
     *
     * 怎么做?
     * 1.开发语言:
     *      就选择普遍使用的java,此外还考虑过groovy, drools可选
     * 2.领域分析和指标管理:
     *      (1)输出:经过分析发现,规则引擎计算出来的可是准入额度利率等标准的授信要素,也可以黑白名单等非授信要素,那么规则引擎的输出就无法抽象建模,所以我们采用Map数据结构来适应上游业务的不同需求
     *      作为模型的标准输出,KV由业务自己进行定义和拆装箱
     *      (2)输入:指标管理:指标就是模型的各种输入数据,这些指标管理要求全局唯一和命名有可读性以达到高度的复用的目的,用大领域+指标含义+具体业务+版本号进行命名,加上中文注释,然后系统自动生成即可
     * 3.动态发布:
     *      (1)采用Java动态代理机制实现动态发布,也就是用classLoader生成Proxy,然后method.invoke (已知相关技术有OSGi、Java插件机制、动态类加载、动态代理、类的动态生成等)
     *      (2)每个规则包,大业务相同的使用一个Git工程,工程里不同字业务的采用Module进行区分; 比如:淘系列就是一个git工程,套系的下天猫就是一个Module,淘宝是另一个Module,1688就是另一个Module,每个Module都可以独立打成规则包
     * 4.版本管理:
     *      规则引擎下的一个规则包,是一个不断升级的过程,我们采用AB机制,引擎中实时在跑两个版本的规则包,一本版本生效,一个版本升级备用
     *      具体来说:新规则包上线,规则引擎会自动发布1.0和1.1两个版本,并且以高版本为生产结果; 但我们升级规则包时候,低版本的1.0升级到1.2-beta,经过现在测试符合预期后,正式升级到1.2.原1.1版本失效,但仍然存在于内存中,准备下一个版本升级
     * 5.数据一致性:
     *      每个规则包对应唯一Id,同一个数据源,同一个id存一高一低两个版本的配置数据,在同一个张表,AB切换的时候采用本地事务,保证升级切换过程中的数据一致性,也就是只会有有一个版本的生效,同时具备回滚能力
     * 6.灰度切流:
     *      AB版本切换过程中,不是一把切的,是会根据每个版本包的配置的灰度规则进行灰度的,我们有按照时间的百分比和用户进行灰度两种策略
     *      具体来说就是: 淘宝商家信用贷款,这个政策模型,1.1版本切换到1.2版本过程中,配置灰度规则[1小时11%,第二小时10%,第三小时30%],跑规则时候,会根据当前时间和模型发布时间做计算,看命中那个贵规则阶梯
     *                  再根据当前用用户id倒数二三位,进行百分比匹配,决定采用哪一个版本的规则包,再去跑规则; (实际两个规则包一起跑,后续讲为什么)
     *
     *                  第二种策略就是根据用户百分比,比如用户id 前1%的先切流,然后手动控制灰度开关,3%,10%等切换过去;
     *                  前一种灰度策略是根据时间推移自动的,后一种是手动的;
     *
     *                  一般老业务升级选择自动的,新业务上线选择手动的;
     *
     * 挑战点或者难点?
     * A:如何保证规则包的动态发布要求?
     *      技术上采用Java动态代理机制实现动态发布
     * B.如何保证规则上线的准确性?
     *      测试角度分析:
     *          保证单元测试覆盖率100%(开发人员)
     *          使用生产流量验证线下测试规则,A版本在跑,B版本升级发布,试用A版本的流量同时输入B版本,让验证结果落地成日志然后回流到离线,政策整体验证整体结果是否符合预期(政策人员)
     *          这就是上述所说的,实际高低两个版本规则包是在内存中串行跑的,因为另一个一般用于测试验证下一个版本规则
     * C:如何保证规则包发布的安全性?
     *      AB两个版本包,同时存在于内存中,切换采用本地事务,保证数据一致性,业务的安全性
     * D:如何保证规则引擎的计算高效性?
     *      对于单个规则包来说,
     *      1.所需要的输入的数据先加工,后使用,按照人维度进行加工或者采集,落到本地库去(单次DAO 就是3-5ms)
     *      2.计算规则加载到内存中,仅输入数据,然后内存if else的路由,内存非IO操作计划没有成本
     *      3.减少多次DAO,避免出现RPC,跨库,跨机房等耗时或者不稳定的操作
     * E:平滑过度切流量
     *     这个比较简单,新老系统双跑,规则模型结果以老系统为主进行比对,以新的结果返回为主,后续核对完成后,灰度切换到新系统,来系统业务下线即可;
     *
     * 支撑业务:
     *     管理网商银行企业信贷70多个规则模型,4核8G300台服务,支撑了所有信贷规则咨询业务,日均QPS4w,压测可达10w,服务千万客户,千亿余额,万亿授信规模
     *
     * 二.离线反向实时调度系统:
     * 背景?
     *  各个业务系统经常有从离线按照规则清洗数据,然后同步到实时库,再使用调度任务批量捞起,实现具体的业务等诉求或者场景,这个任务可能是定期的,也可能是一次性的
     *  举例:比如离线清洗名单,清洗结束后实时发短信; 离线计算客户的应还账款,逾期金额每天发送等;给系统中参与了双十一活动的的指定用户,发一条营销短信,告诉他们的双十二的活动情况?
     *  包括我们规则引擎系统本身,也逐渐发展了很多这样的业务,每次都是离线清洗,实时任务捞取,硬编码的成分偏多,没有规范化抽象化流程
     *
     *  承担离线到实时的数据同步职能,通过离线数据同步+调度任务+SPI的方式,实现离线数据同步到业务系统的基础能力;
     *
     *
     * 目标?
     *  抽象化一个离线反向调度系统,让(1)离线数据清洗->(2)数据同步到实时库->(3)批量调度->(4)调度后实现具体业务,这个四大步骤,业务仅关注业务两端,不关注中间过程,降本增效
     *
     * 怎么做?
     *  规范领域模型:
     *      (1)进过分析总结,离线数据反向调度系统,其实就是围绕着业务数据流转展开的,那么我们定义一个叫做SYNC_DOMAIN的数据模型,
     *      (2)它由BIZ_ID,BIZ_TYPE,STATUS,SCHEDULE_RATE,BIZ_DATA这几个核心领域对象组成,分别代表了XXX,在ER关系上,我们使用主任务和子任务1对多的模型,即一个业务ID,对应100张表存子任务,保证并发性,数据维度一般是用户维度
     *      (3)其中最重要的是业务ID和业务数据,一个用于区分业务类型,一个是业务驱动所需要的内部数据,也就是实时做具体业务所需要离线加工出来的数据,数据结构使用MAP
     *  流程抽象和统一:
     *      固定步骤: (1)离线数据清洗->(2)数据同步到实时库->(3)批量调度->(4)调度后实现具体业务
     *
     *      用户侧 A:用户固定实现步骤(1)数据离线清洗,然后同步到我们的系统的固定表中,其实就是同步到上述数据模型下的BIZ_DATA字段下成为一个JSON
     *              其中我们实现了一个离线依赖包,用于对用户屏蔽掉数据库连接,插入等操作,让业务专注于写自己的清洗SQL,加工成JSON即可,当然,业务还要办法一个全局唯一的BIZ_ID进行区分
     *            B:用户再固定实现步骤(4),即在业务末端继承实现我们的通用接口,然后获取离线加工的数据,用于做自己的业务,完成后就等着离线加工任务完成,就会哟调度任务RPC进行业务驱动
     *      平台侧:
     *            C:具体技术是SPI,他是一种服务发现机制,针对接口和实现进行解耦,原理是基于查找和装载机制的动态加载方式,当然,业务系统在JVM启动的时候就要上述分配的全局唯一BIZ_ID进行付业务SPI注册
     *            D:批量调度任务的实现,我们和借呗进行了共建
     *             相比传统的调度任务,在任务拉起的时候可能出现CPU尖刺,导致服务器资源没有得到最大化的利用而且有宕机的风险
     *
     * 挑战点或者难点?
     *  离线数据清洗和同步不是难点,这是阿里云提供的基础设施
     *  A:SPI技术的使用
     *      他是一种服务发现机制,针对接口和实现进行解耦,原理是基于查找和装载机制的动态加载方式,当然,业务系统在JVM启动的时候就要上述分配的全局唯一BIZ_ID进行付业务SPI注册
     *  B:调度任务并发和高效
     *     基本流程:
     *      (1)调度器: 调度线程抢锁->捞取数据->时间轮配合进行(规划,分组,对齐和循环) -> 提到到线程池
     *      (2)执行器: 使用执行队列和回调队列,进行任务执行和回调,执行完成后如果有子任务,则放到线程池中继续,没有则进行任务状态标记
     *     并发: 上述调度器和执行器都可以分布式部署,采用执行器注册到调度器,也就是协调者+心跳探活的方式,来保证集群的可用,从而保证整体并发
     *     锁机制: 在调度啥时候,获取任务或者子任务粒度的排他锁,在并发时保证安全和幂等
     *     时间轮算法: 巧妙的使用时间进行任务的规划,分组,对齐和循环
     *     令牌桶算法: 集群机器调度时候,同时需要根据令牌桶进行控制,保证CPU不出现尖刺等,实现服务资源的最大化利用,并且可以细化到业务id维度
     *     多策略支持:(这里我们在业务上并不敏感,一般就采用默认策略,因为从离线到实时调度,并不需要非常实时)
     *          任务阻塞策略:丢弃策略
     *          任务执行策略:最不经常使用策略
     *   C:支撑业务::
     *      支撑了网商10多条业务线,日均上万QPS和上亿数据的业务,4核8G集群部署8台机器,就能向外调度近10W的QPS,经常把下游系统压到限流预警,然后手动调整令牌桶
     *
     *
     *     https://zhuanlan.zhihu.com/p/587543371
     *     https://blog.csdn.net/qq_37871033/article/details/128439358
     *
     *
     * 三.系统的容灾性建设
     *    (主要体现在高可用,高并发,高性能中的高可用容灾上,达到了数据库集群挂,预警后分钟级切数据库集群副本库,FailOver
     *      除了分布式,负载均衡,灰度发布,监控预警,搞性能的中间件和dataBase等数据存储介质等互联网通用能力,我们就不展开讲了,我么重点讲一下
     *   A: 数据库集群挂了,如何确保业务不跌零,可用率大于50%?
     *       把数据写入源分为实时和离线两部部分
     *       实时(业务系统写入的,空间换可用):10库100表在一个OB集群,现在分散到10个集群上,每个集群仅存一个库,这个样集群怪最多影响10%
     *       离线(离线SQL同步的,多副本换可用):离线同步到实时库时候同步两份到两个集群,流量50%均分到这两个集群,一个集群挂了,预警出来,分钟级切换手动切换到另一个集群(去年12月产生了一次作用,和买保险一样)
     *       难点:思路很清晰也很简单,难点在于如何在日均4wQPS一秒不停服的前提下把所有数据挪动到别的集群上去?
     *       做法: 双写,灰度,回滚
     *              1.初始化同步数据单向(小时级),A集群到B集群之间有延迟,流量打到A集群上,以A集群返回为准
     *              2.选择夜晚1点业务低峰期,QPS降低上千级别,开启双向同步且强一致(分钟级),此时要求主副本和备副本都同步完成,业务才返回,此时会有牺牲一定的耗时(交易增加100ms左右),但是可以接受
     *              3.流量灰度切换过去,1%,3%,5%,10%,以后隔天切10%过去,次日观察业务是否正常,有问题就回切,没问题就继续切,直到100%,前后耗时2个月
     * 挑战点或者难点:
     *     安全要求高,不停服,相当于把网商银行融资平台在不停服的前提下把左右数据辗转腾挪一遍,确保数据准确
     *
     *
     *
     * 美团的金融项目总结
     *
     *
     */
}